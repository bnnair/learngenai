{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lang_funcs import *\n",
    "from langchain_community.llms import ollama\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspaces\\AI-Projects\\opensourceRAG\\raglib\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "d:\\workspaces\\AI-Projects\\opensourceRAG\\raglib\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loading orca-mini from Ollama\n",
    "llm = ollama.Ollama(model=\"orca-mini\", temperature=0)\n",
    "\n",
    "# Loading the Embedding Model\n",
    "embed = load_embedding_model(model_path=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and splitting the documents\n",
    "docs = load_pdf_data(\n",
    "    file_path=\"data/ml_book.pdf\")\n",
    "documents = split_docs(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vectorstore\n",
    "vectorstore = create_embeddings(documents, embed)\n",
    "\n",
    "# converting vectorstore to a retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the prompt from the template which we created before\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Creating the chain\n",
    "chain = load_qa_chain(retriever, llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspaces\\AI-Projects\\opensourceRAG\\raglib\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When comparing different machine learning models, it is important to consider their accuracy or\n",
      "precision. Accuracy refers to the number of true positives divided by the total number of\n",
      "predictions made. Precision, on the other hand, refers to the number of true positives divided by\n",
      "the total number of positive predictions (including both true positives and false positives).  To\n",
      "determine the accuracy of a model, we need to compare the number of true positives and the number of\n",
      "positive predictions made by the model. For example, if a model made 5 positive predictions in the\n",
      "first batch, 4 of which were correct, then the precision of the model would be 80%. If we make the\n",
      "same prediction for the second batch, but all of them are incorrect, then the precision of the model\n",
      "would be 0%.  To determine the accuracy of a model, we can also compare the number of true positives\n",
      "and the number of positive predictions made by the model. This gives us an idea of how well the\n",
      "model is performing. However, it does not give us information about the precision of the model.  The\n",
      "AIC (Akaike information criterion) and BIC (Bayesian information criterion) are two measures of how\n",
      "well a model fits the data. The AIC is defined as the sum of the log-likelihood ratio scores for all\n",
      "parameters in the model, while the BIC is defined as the sum of the log-likelihood ratio scores for\n",
      "all parameters in the model, plus one more term that penalizes the number of parameters learned by\n",
      "the model.  The BIC usually tends to select a simpler model than the AIC, but it does not fit the\n",
      "data quite as well. This is because the BIC penalizes models with more parameters to learn (e.g.,\n",
      "more clusters), while the AIC rewards models that fit the data well. When comparing different\n",
      "machine learning models, we should consider both their accuracy and the complexity of the model.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"how accuracy of different models are determined. Explain in detail.\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(\"explain in detail decision trees, include all the information from training the model to model performance compared to other ML models\", chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raglib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
